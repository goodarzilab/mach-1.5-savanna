
{
#NOTE: when using srun_launcher, 
# -DO NOT set deepspeed_slurm to true
# -DO NOT set master_port as this will be generated automatically in the SLURM script
"use_srun_launcher": true,
"deepspeed_slurm": false,
"use_wandb": false,

"make_vocab_size_divisible_by": 8,
"num_layers": 50,
"hidden_size": 8192,
"num_attention_heads": 64,
"num_groups_hyena": 8192,
"num_groups_hyena_medium": 512,
"num_groups_hyena_short": 512,
"num_groups_hyena_mlp": 512,
"operator-config": [ 
[["hyena_se"], 1], 
[["hyena_mr"], 1], 
[["hyena"], 1],
[["flash_v2"], 1],
[["hyena_se"], 1], 
[["hyena_mr"], 1], 
[["hyena"], 1],
[["hyena_se"], 1], 
[["hyena_mr"], 1], 
[["hyena"], 1],
[["flash_v2"], 1],
[["hyena_se"], 1], 
[["hyena_mr"], 1], 
[["hyena"], 1],
[["hyena_se"], 1], 
[["hyena_mr"], 1], 
[["hyena"], 1],
[["flash_v2"], 1],
[["hyena_se"], 1], 
[["hyena_mr"], 1], 
[["hyena"], 1],
[["hyena_se"], 1], 
[["hyena_mr"], 1], 
[["hyena"], 1],
[["flash_v2"], 1],
[["hyena_se"], 1], 
[["hyena_mr"], 1], 
[["hyena"], 1],
[["hyena_se"], 1], 
[["hyena_mr"], 1], 
[["hyena"], 1],
[["flash_v2"], 1],
[["hyena_se"], 1], 
[["hyena_mr"], 1], 
[["hyena"], 1],
[["flash_v2"], 1],
[["hyena_se"], 1], 
[["hyena_mr"], 1], 
[["hyena"], 1],
[["hyena_se"], 1], 
[["hyena_mr"], 1], 
[["hyena"], 1],
[["flash_v2"], 1],
[["hyena_se"], 1], 
[["hyena_mr"], 1], 
[["hyena"], 1],
[["hyena_se"], 1], 
[["hyena_mr"], 1], 
[["hyena"], 1],
[["flash_v2"], 1], 
],

# These kernels will be also autotuned and activated
"use_cgcg": false,
"use_cgcg_short": false,
"use_cgcg_mlp": false,

# Tune to target sequence length e.g., 8192
"seq_length": 8192,
"max_position_embeddings": 8192,

"hyena_mr_len": 128,  # default is null
"log_attn_norms": false,
"pos_emb": "rotary",
"rotary_emb_base": 1000000,
"rotary_pct": 1,
"prenorm": true,
"postnorm": false,
"pre_mlp_norm": true,  
"outer_mlp_norm": false,
"no_weight_tying": false,
"gpt_j_residual": false,
"normalize_hyena_filters": false,
"short-conv-L": 3, 
"hyena_filter_fast_decay": 0.3,
"hyena_filter_slow_decay": 1.2,
"hyena_filter_w": 14, 
"hyena_filter_cls": "implicit_modal",
"hyena_medium_filter_cls": "explicit_single_decay",  
"explicit_filter_decay_preset": "weak",
"hyena_filter_order": 16,
"hyena_filter_wd": 0.,
"use_fast_heads": false,
"use_slow_heads": false,
"use-hyena-filter": true,
"output_layer_parallelism": "column",
"bias_dropout_fusion": false,
"norm": "rmsnorm",
"rms_norm_epsilon": 1.0e-6,
"identity_mlp": false,
"activation": "gelu",
  "mlp_type": "llama",
  "scaled-upper-triang-masked-softmax-fusion": true,
  "bias-gelu-fusion": false,
  "init_method": "small_init",
  "output_layer_init_method": "wang_init",
  "optimizer": {
    "type": "Adam",
    "params": {
      "lr": 0.0003,
      "betas": [0.9, 0.95],
      "eps": 1.0e-8,
    }
  },
  "min_lr": 0.00003,

  "pipe_parallel_size": 0,
  # We use ZeRO-3 + TP, or ZeRO-3 + TP + CP
  # TP world size to be autotuned on the cluster
  "model_parallel_size": 2,
  "sequence_parallel": true,
    "zero_optimization": {
    "stage": 3,
    "prefetch_bucket_size": 500000000,
    "max_live_parameters": 1000000000,
    "allgather_partitions": True,
    "allgather_bucket_size": 500000000,
    "overlap_comm": True,
    "reduce_scatter": True,
    "reduce_bucket_size": 500000000,
    "contiguous_gradients": True,
    "cpu_offload": false,
    "param_persistence_threshold": 0,
  },
  "train_micro_batch_size_per_gpu": 2,
  "gradient_accumulation_steps": 1,
  "data-impl": "mmap",
  "checkpoint-activations": true,
  "checkpoint-num-layers": 4,
  "partition-activations": false,
  "synchronize-each-layer": false,
  "gradient_clipping": 1.0,
  "weight-decay": 0.1,
  "hidden-dropout": 0.0,
  "attention-dropout": 0.0,
  "precision": "bfloat16",
  "bf16": {
  "enabled": true
  },
  "train-iters": 20,
  "lr-decay-iters": 20,
  "distributed-backend": "nccl",
  "lr-decay-style": "cosine",
  "warmup": 0.005,
  "checkpoint-factor": 2500,
  "extra_save_iters": [100],
  "eval-interval": 200,
  "eval-iters": 20,
  "log-interval": 5,
  "steps_per_print": 5,
  "keep-last-n-checkpoints": 100,
  "wall_clock_breakdown": false,

  "save": "/home/zymrael/checkpoints/evo2/evo2_test", # change this
  "tokenizer_type": CharLevelTokenizer,   
  "use_fp8_input_projections": true,
  "use_fp8_output_projections": true,
  "use_fp8_mlp_projections": true,
  "use_fp8_norm": true,
  "checkpoint_strict_load": false,
  "make_gated_mlp_multiple_of": 128,
  "materialize_attn_mask": false,  # default false, to save memory
  "fast_conv_proj": true,   
  "hyena_se_len": 7,
  'to_upper': "normalized_weighted",
'mask_loss_control_tags': true,
  "lowercase_loss_reweighting": 0.1,  
}